# Crawler
Python.Task

Автор: Суровнев Владислав

Название задачи: Краулер

Поиско́вый ро́бот, или веб-кра́улер — программа, являющаяся составной частью поисковой системы и предназначенная для перебора страниц Интернета с целью занесения информации о них в базу данных поисковика.

По принципу действия, «паук» напоминает обычный браузер. Он анализирует содержимое страницы, сохраняет его в некотором специальном виде на сервере поисковой машины, и отправляется по ссылкам на следующие страницы. Порядок обхода страниц, частота визитов, защита от зацикливания, а также критерии выделения значимой информации определяются алгоритмами информационного поиска. В большинстве случаев переход от одной страницы к другой осуществляется по ссылкам, содержащимся на первой и последующих страницах.

Реализованные требования:
- не используется scrapy
- парсинг с помощью html.parser
- сохранение html страниц на диск, не скачивать повторно
- переход по ссылкам в рамках текущего домена / списка доменов
- скачивание в несколько потоков
- возможность докачки